<!DOCTYPE html>
<html>

<head>
  <meta charset="utf-8">
  <link rel="author" href="https://henghuiding.com/MOVE">
  <title>MOVE
  </title>
  <meta name="description" content="MOVE: Motion-Guided Few-Shot Video Object Segmentation">
  <meta name="keywords"
    content="MOVE; Kaining Ying; Hengrui Hu; Henghui Ding; Fudan University; Few-shot; Video Object Segmentation; Motion; Dataset">

  <meta name="description"
    content="MOVE: Motion-Guided Few-Shot Video Object Segmentation">
  <meta property="og:title"
    content="MOVE: Motion-Guided Few-Shot Video Object Segmentation" />
  <meta property="og:description"
    content="MOVE: Motion-Guided Few-Shot Video Object Segmentation" />

  <meta name="twitter:title"
    content="MOVE: Motion-Guided Few-Shot Video Object Segmentation">
  <meta name="twitter:description"
    content="MOVE: Motion-Guided Few-Shot Video Object Segmentationn">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro" rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
  <meta name="google-site-verification" content="RHqlM-yRssUYgbykhtd0uguPnqkhTvwJw-aLE04B4KQ" />


  <style>
    body {
      min-width: 420px;
      margin: 0;
      padding: 20px;
      box-sizing: border-box;
    }

    .image-pairs-container {
      display: flex;
      flex-wrap: wrap;
      gap: 20px;

      justify-content: center;

    }


    .pair {
      display: flex;
      flex-direction: column;

      flex-shrink: 0;

      width: 175px;

      background-color: #fff;
      padding: 10px;
      border-radius: 8px;
      box-shadow: 0 2px 5px rgba(0, 0, 0, 0.1);
    }

    .pair h5 {
      margin: 0 0 5px 0;
      font-size: 18px;
      text-align: center;
      color: #333;
      white-space: pre-wrap;
    }

    .pair img {
      width: 200px;
      height: auto;
      display: block;
    }

    @media (max-width: 420px) {
      .image-pairs-container {
        overflow-x: auto;
      }
    }
  </style>

</head>

<body>
  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title">MOVE: Motion-Guided Few-Shot Video Object Segmentation
            </h1>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                <a target="_blank">Kaining Ying</a><sup>*</sup>,&nbsp;</span>
              <span class="author-block">
                <a target="_blank">Hengrui Hu</a><sup>*</sup>,&nbsp;</span>
              <span class="author-block">
                <a href="https://henghuiding.com/" target="_blank">Henghui Ding</a><sup>‚úâÔ∏è</sup></span>
            </div>
            <div class="is-size-5 publication-authors">
              <span class="author-block">
                Fudan University, China
            </div>

             <div class="is-size-5 publication-authors">  
                <span class="author-block">
                <font color="#FF6403"><b>ICCV 2025, Honolulu, Hawai'i</b></font></span>
            </div>
            <div class="is-size-6 publication-authors">
              <span class="author-block">
                <sup>*</sup> Equal contribution, <sup>‚úâÔ∏è</sup> Corresponding author
              </span>
            </div>

            <div class="column has-text-centered">
              <div class="publication-links">
                <span class="link-block">
                  <a href="https://arxiv.org/abs/xxxx"  target="_blank" class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="ai ai-arxiv"></i>
                    </span>
                    <span>arXiv</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/FudanCVL/MOVE" class="external-link button is-normal is-rounded is-dark">
                    <span>üî•Code</span>
                  </a>
                </span>

                <span class="link-block">
                  <a href="https://github.com/FudanCVL/MOVE"
                    class="external-link button is-normal is-rounded is-dark">
                    <span>üî•Dataset</span>
                  </a>

                </span>
              </div>
            </div>
          </div>
        </div>
      </div>
    </div>
  </section>

  </section>

  <section class="hero teaser">
    <div class="container is-max-desktop">
      <div class="hero-body">
        <center><img src="static/images/teaser.jpg" border="0" width="100%"></center>

        <div align="center">
          <p style="text-align:justify; text-justify:inter-ideograph;width:100%"><b>Figure 1</b>. We propose a new benchmark for <b>MO</b>tion-guided Few-shot <b>V</b>ideo object s<b>E</b>gmentation (<b>MOVE</b>). In this example, given two support videos showing distinct motion patterns (S1: Cristiano Ronaldo's signature celebration, S2: hugging), our benchmark aims to segment target objects in the query video that perform the same motions as in the support videos. <b>MOVE</b> provides a platform for advancing few-shot video analysis and perception by enabling the segmentation of diverse objects that exhibit the same motions.</p>
        </div><br>

      </div>
  </section>

  <section class="section hero is-light">
    <div class="container is-max-desktop">
      <div class="columns is-centered has-text-centered">
        <div class="column is-four-fifths">
          <h2 class="title is-3">Abstract</h2>
          <div class="content has-text-justified">
            <p style="text-align:justify; text-justify:inter-ideograph;">
              This work addresses motion-guided few-shot video object segmentation (FSVOS), which aims to segment dynamic objects in videos based on a few annotated examples with the same motion patterns. Existing FSVOS datasets and methods typically focus on object categories, which are static attributes that ignore the rich temporal dynamics in videos, limiting their application in scenarios requiring motion understanding. To fill this gap, we introduce MOVE, a large-scale dataset specifically designed for motion-guided FSVOS. Based on MOVE, we comprehensively evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings. Our results reveal that current methods struggle to address motion-guided FSVOS, prompting us to analyze the associated challenges and propose a baseline method, Decoupled Motion-Appearance Network (DMA). Experiments demonstrate that our approach achieves superior performance in few-shot motion understanding, establishing a solid foundation for future research in this direction.
            </p>
          </div>
        </div>
      </div>
    </div>
  </section>

  <section class="section" id="MOVE-Visualization">
    <div class="container is-max-desktop content">
      <h2 class="title">Dataset: MOVE</h2>
      <h3 class="title">Dataset Statistics</h3>
      <div class="table-container" style="text-align:center;">
        <table class="table is-bordered is-striped is-narrow is-hoverable is-fullwidth">
          <thead>
            <tr>
              <th>Dataset</th>
              <th>Venue</th>
              <th>Label Type</th>
              <th>Annotation</th>
              <th>Support Type</th>
              <th>Categories</th>
              <th>Videos</th>
              <th>Objects</th>
              <th>Frames</th>
              <th>Masks</th>
            </tr>
          </thead>
          <tbody>
            <tr>
              <td>FSVOD-500</td>
              <td>ECCV'22</td>
              <td>Object</td>
              <td>Box</td>
              <td>Image</td>
              <td>500</td>
              <td>42,72</td>
              <td>96,609</td>
              <td>104,495</td>
              <td>104,507</td>
            </tr>
            <tr>
              <td>YouTube-VIS</td>
              <td>ICCV'19</td>
              <td>Object</td>
              <td>Mask</td>
              <td>Image</td>
              <td>40</td>
              <td>2,238</td>
              <td>3,774</td>
              <td>61,845</td>
              <td>97,110</td>
            </tr>
            <tr>
              <td>MiniVSPW</td>
              <td>IJCV'25</td>
              <td>Object</td>
              <td>Mask</td>
              <td>Image</td>
              <td>20</td>
              <td>2,471</td>
              <td>-</td>
              <td>541,007</td>
              <td>-</td>
            </tr>
            <tr>
              <td><b>MOVE (ours)</b></td>
              <td>ICCV'25</td>
              <td>Motion</td>
              <td>Mask</td>
              <td>Video</td>
              <td>224</td>
              <td>4,300</td>
              <td>5,135</td>
              <td>261,920</td>
              <td>314,619</td>
            </tr>
          </tbody>
        </table>
      </div>
      <p style="text-align:center;font-size:0.95em;"><b>Table 1</b>: Comparison of MOVE with existing video object segmentation datasets.</p>
  
      <p style="text-align:justify; text-justify:inter-ideograph;">As shown in Table 1, our MOVE benchmark contains 224 action categories across four domains (daily actions, sports, entertainment activities, and special actions), with 4,300 video clips, 5,135 moving objects, 261,920 frames, and 314,619 mask annotations. Compared to existing object-centric datasets, MOVE features video-level support samples and motion-based categories, while maintaining comparable scale in terms of videos and annotations. Each video clip is equipped with high-quality pixel-level mask annotations, capturing diverse scenes, subjects (person, vehicle, animal, etc.), and motion complexities. For more detailed statistics and analysis, please refer to the supplementary materials.</p>

      <h3 class="title">Visualization of Different Motions</h3>
      
      <div style="text-align:center; margin: 2em 0;">
        <div class="image-pairs-container">
          <div class="pair">
            <h5><b>Cross</b></h5>
            <img src="static/demo/cross.webp" alt="Cross motion" width="175" height="175" />
          </div>
          <div class="pair">
            <h5><b>Heimlich</b></h5>
            <img src="static/demo/heimlich.webp" alt="Heimlich" width="175" height="175" />
          </div>
          <div class="pair">
            <h5><b>Hugging</b></h5>
            <img src="static/demo/hugging.webp" alt="Hugging motion" width="175" height="175" />
          </div>
          <div class="pair">
            <h5><b>Spinning Plate</b></h5>
            <img src="static/demo/spinning_plate.webp" alt="Spinning plate" width="175" height="175" />
          </div>
        </div>
        <div class="image-pairs-container">
          <div class="pair">
            <h5><b>Stacking Dice</b></h5>
            <img src="static/demo/stack_dice.webp" alt="Stacking dice" width="175" height="175" />
          </div>
          <div class="pair">
            <h5><b>Waacking</b></h5>
            <img src="static/demo/waack.webp" alt="Waacking dance" width="175" height="175" />
          </div>
          <div class="pair">
            <h5><b>Yoga Pigeon</b></h5>
            <img src="static/demo/yoga_pigeon.webp" alt="Yoga pigeon pose" width="175" height="175" />
          </div>
          <div class="pair">
            <h5><b>Siu</b></h5>
            <img src="static/demo/siu.webp" alt="Siue" width="175" height="175" />
          </div>
        </div>
        <p style="text-align:center; font-size:0.95em; margin-top:15px;"><b>Figure 2</b>: Examples of diverse motion categories in the MOVE dataset.</p>
      </div>
      <h3 class="title">Visualization of Different Objects with Same Motion</h3>
      
      <div style="text-align:center; margin: 2em 0;">
        <div class="image-pairs-container">
          <div class="pair">
            <h5><b>Hugging</b></h5>
            <img src="static/demo2/hugging_person.webp" alt="Person hugging" width="175" height="175" />
            <p>Person</p>
            <img src="static/demo2/hugging_fox.webp" alt="Fox hugging" width="175" height="175" />
            <p>Fox</p>
          </div>
          <div class="pair">
            <h5><b>Metamorphosis</b></h5>
            <img src="static/demo2/metamorphosis_person.webp" alt="Person metamorphosis" width="175" height="175" />
            <p>Person</p>
            <img src="static/demo2/metamorphosis_horse.webp" alt="Horse metamorphosis" width="175" height="175" />
            <p>Horse</p>
          </div>
          <div class="pair">
            <h5><b>Shake</b></h5>
            <img src="static/demo2/shake_persons.webp" alt="Person shaking" width="175" height="175" />
            <p>Person</p>
            <img src="static/demo2/shake_animals.webp" alt="Animal shaking" width="175" height="175" />
            <p>Animal</p>
          </div>
          <div class="pair">
            <h5><b>Siu</b></h5>
            <img src="static/demo2/siu_person.webp" alt="Person siu" width="175" height="175" />
            <p>Person</p>
            <img src="static/demo2/siu_robot.webp" alt="Robot siu" width="175" height="175" />
            <p>Robot</p>
          </div>
        </div>
        <p style="text-align:center; font-size:0.95em; margin-top:15px;"><b>Figure 3</b>: Examples of same motion performed by different object categories in the MOVE dataset.</p>
      </div>

  </section>




  <section class="hero teaser">
    <div class="container is-max-desktop">
      <h2 class="title">2. Baseline Method: DMA</h2>
        <br>
      <div class="hero-body">
        <center>
          <div style="display: flex; justify-content: space-between; align-items: center; width: 100%;">
            <img src="static/images/overview.jpg" border="0" style="width: auto; max-width: 48%;">
            
            <div style="width: 1px; height: 260px; background-color: #cccccc;"></div>
            
            <img src="static/images/dma.jpg" border="0" style="width: auto; max-width: 48%;">
          </div>
        </center>

        <div align="center">
          <p style="text-align:justify; text-justify:inter-ideograph;width:100%;font-size:0.95em;"><b>Figure 3</b>. 
            The architecture of our proposed <b>DMA</b> (Decoupled Motion-Appearance Network). The method consists of five main components: 1) a shared encoder that extracts multi-scale features from both support and query video frames, 2) a proposal generator that produces coarse mask proposals for the query video, 3) a shared DMA module (right) for extracting decoupled motion-appearance prototypes, 4) a prototype attention module that facilitates interaction between support and query prototypes, and 5) a mask decoder that generates the final segmentation masks for the query video. Given a support video with corresponding masks and a query video, our goal is to segment objects in the query video that exhibit the same motion pattern as objects in the support video.
          </p>
        </div>
      </div>

  </section>


  <section class="section" id="Camera_Control">
    <div class="container is-max-desktop content">
      <h2 class="title">Experiments</h2>
      <h3 class="title">Benchmark Results</h3>
      <div class="table-container">
        <center>
          <img src="static/images/os.jpg" alt="OS Results" width="100%" />
          <p style="text-align:justify;font-size:0.95em;"><b>Table 3</b>. Main results on MOVE benchmark with overlapping split (OS) setting. Bold and underlined indicate the largest and second largest values under the same backbone, respectively. VSwin-T indicates VideoSwin-T backbone.</p>
        </center>
      </div>
      
      <div class="table-container">
        <center>
          <img src="static/images/ns.jpg" alt="NS Results" width="100%" />
          <p style="text-align:center;font-size:0.95em;"><b>Table 4</b>. Main results on MOVE benchmark with non-overlapping split (NS) setting.</p>
        </center>
      </div>

      <p style="text-align:justify;">We evaluate 6 state-of-the-art methods from 3 different related tasks across 2 experimental settings on our MOVE benchmark. The two settings are Overlapping Split (OS) and Non-overlapping Split (NS), which differ in how motion categories are distributed across folds for cross-validation. OS allows some motion overlap between folds while NS ensures completely separate motion categories between folds, creating a more challenging scenario. Our proposed DMA method achieves superior performance in few-shot motion understanding compared to existing approaches in both settings.</p>

      <h3 class="title">Qualitative Results</h3>
      <div class="table-container">
        <center>
          <img src="static/images/comparision.jpg" alt="OS Results" width="100%" />
          <p style="text-align:justify;font-size:0.95em;"><b>Figure 4</b>. Qualitative comparison of representative cases from MOVE between baseline methods, DANet and HPAN, and our proposed DMA. The examples show: different object categories performing the same action (cat and person playing drums); temporally correlated motions with fingers transitioning between pinching and opening positions; and a challenging case with misleading background where football is played on a basketball court.</p>
        </center>
      </div>
      
      </section>

  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      Please consider to cite MOVE if it helps your research.
      <pre><code>@inproceedings{ying2025move,
  title={{MOVE}: {M}otion-{G}uided {F}ew-{S}hot {V}ideo {O}bject {S}egmentation}, 
  author={Kaining Ying and Hengrui Hu and Henghui Ding},
  year={2025},
  booktitle={ICCV}
}</code></pre>
    </div>
  </section>






  <a href="https://clustrmaps.com/site/1c7ch" title="ClustrMaps" target="_blank"><img
      src="//www.clustrmaps.com/map_v2.png?d=A4O8vj6nHKr-wzDSok1gt01FP_r9dc5COhnY6v-ztGY&cl=ffffff" height="1" width="1"
      / style="display:block;margin-top:5px;margin-bottom:0px;margin-left:auto;text-align:right"></a>
  <footer class="footer">
    <div class="container">
      <div class="columns is-centered">
        <div class="column is-8">
          <div class="content">
            <p>
              <center>
                <font size=2>¬© FudanCVL | Last updated: 28/07/2025</font>
              </center>
            </p>
          </div>
        </div>
      </div>
    </div>
  </footer>

</body>

</html>